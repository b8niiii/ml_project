    "# Cross-validated hyperparameter tuning for Model B\n",
    "# This cell performs a grid search with StratifiedKFold cross-validation\n",
    "# over Model B hyperparameters. After selecting the best combination by\n",
    "# mean validation accuracy, the model is retrained on the full training\n",
    "# pool and evaluated on the untouched test set. A short Markdown report is\n",
    "# generated at the end with the chosen values and final metrics.\n",
    "import os, time, random, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras import layers\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build training pool (train + val folders)\n",
    "# ------------------------------------------------------------------\n",
    "train_root = Path('data/processed/train')\n",
    "val_root   = Path('data/processed/val')\n",
    "all_paths, all_labels = [], []\n",
    "for idx, name in enumerate(class_names):\n",
    "    all_paths += sorted(str(p) for p in train_root.joinpath(name).glob('*.png'))\n",
    "    all_paths += sorted(str(p) for p in val_root.joinpath(name).glob('*.png'))\n",
    "    all_labels += [idx] * (len(list(train_root.joinpath(name).glob('*.png'))) + len(list(val_root.joinpath(name).glob('*.png'))))\n",
    "all_paths = np.array(all_paths)\n",
    "all_labels = np.array(all_labels)\n",
    "print(f'Training pool: {len(all_paths)} images')\n",
    "\n",
    "# confirm test set size\n",
    "_test_paths = list(Path('data/processed/test').glob('*/*.png'))\n",
    "print(f'Test set: {len(_test_paths)} images')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper to build datasets from explicit filepaths/labels\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def make_ds_from_paths(paths, labels, img_size, batch_size, augment=False, shuffle=False, seed=SEED):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(paths), seed=seed)\n",
    "    def _load(path, label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.resize(img, img_size)\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        return img, label\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if augment:\n",
    "        aug = tf.keras.Sequential([\n",
    "            layers.RandomFlip('horizontal', seed=seed),\n",
    "            layers.RandomRotation(0.05, seed=seed),\n",
    "            layers.RandomTranslation(0.02,0.02, seed=seed),\n",
    "            layers.RandomZoom(0.05, seed=seed)\n",
    "        ])\n",
    "        ds = ds.map(lambda x,y: (aug(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "# ------------------------------------------------------------------\n",
    "# Tunable Model B builder\n",
    "# ------------------------------------------------------------------\n",
    "def build_model_b_tunable(input_shape, num_classes, dropout_conv, dropout_head, weight_decay):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for f in [32,64,128]:\n",
    "        shortcut = x\n",
    "        x = layers.Conv2D(f,3,padding='same',use_bias=False,\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(f,3,padding='same',use_bias=False,\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != f:\n",
    "            shortcut = layers.Conv2D(f,1,padding='same',use_bias=False,\n",
    "                                     kernel_initializer='he_normal',\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "        x = layers.Add()([x, shortcut])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "        x = layers.SpatialDropout2D(dropout_conv)(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_head)(x)\n",
    "    x = layers.Dense(128, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = layers.Dropout(dropout_head)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs, outputs, name='ModelB_Tunable')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Grid search setup\n",
    "# ------------------------------------------------------------------\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-4, 1e-3],\n",
    "    'epochs':       [20, 30, 40],\n",
    "    'batch_size':   [32, 64],\n",
    "    'dropout_conv': [0.10, 0.20],\n",
    "    'dropout_head': [0.25, 0.40],\n",
    "    'weight_decay': [1e-4, 5e-5],\n",
    "}\n",
    "combinations = list(itertools.product(\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['epochs'],\n",
    "    param_grid['batch_size'],\n",
    "    param_grid['dropout_conv'],\n",
    "    param_grid['dropout_head'],\n",
    "    param_grid['weight_decay']\n",
    "))\n",
    "print(f'Total combinations: {len(combinations)}')\n",
    "\n",
    "cv_records = []\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for (lr, epochs, bs, d_conv, d_head, wd) in combinations:\n",
    "    fold = 0\n",
    "    for train_idx, val_idx in skf.split(all_paths, all_labels):\n",
    "        fold += 1\n",
    "        train_paths, val_paths = all_paths[train_idx], all_paths[val_idx]\n",
    "        train_labels, val_labels = all_labels[train_idx], all_labels[val_idx]\n",
    "        train_ds = make_ds_from_paths(train_paths, train_labels, IMG_SIZE, bs, augment=True, shuffle=True, seed=SEED)\n",
    "        val_ds   = make_ds_from_paths(val_paths,   val_labels,   IMG_SIZE, bs, augment=False, shuffle=False)\n",
    "        model = build_model_b_tunable(input_shape, num_classes, d_conv, d_head, wd)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=opt,\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "        ]\n",
    "        start = time.time()\n",
    "        history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, verbose=0, callbacks=callbacks)\n",
    "        train_time = time.time() - start\n",
    "        best_epoch = int(np.argmax(history.history['val_accuracy']) + 1)\n",
    "        best_val_acc = float(np.max(history.history['val_accuracy']))\n",
    "        best_val_loss = float(history.history['val_loss'][best_epoch-1])\n",
    "        cv_records.append({\n",
    "            'lr': lr,\n",
    "            'epochs': epochs,\n",
    "            'batch_size': bs,\n",
    "            'dropout_conv': d_conv,\n",
    "            'dropout_head': d_head,\n",
    "            'weight_decay': wd,\n",
    "            'fold': fold,\n",
    "            'val_acc': best_val_acc,\n",
    "            'val_loss': best_val_loss,\n",
    "            'best_epoch': best_epoch,\n",
    "            'train_time_s': train_time\n",
    "        })\n",
    "cv_results_df = pd.DataFrame(cv_records)\n",
    "cv_summary = (cv_results_df\n",
    "    .groupby(['lr','epochs','batch_size','dropout_conv','dropout_head','weight_decay'])\n",
    "    .agg(val_acc_mean=('val_acc','mean'),\n",
    "         val_acc_std=('val_acc','std'),\n",
    "         val_loss_mean=('val_loss','mean'),\n",
    "         val_loss_std=('val_loss','std'))\n",
    "    .reset_index())\n",
    "cv_summary = cv_summary.sort_values('val_acc_mean', ascending=False)\n",
    "cv_summary.to_csv('cv_results_model_b.csv', index=False)\n",
    "print('\\nTop 5 hyperparameter combinations by mean val_acc:')\n",
    "print(cv_summary.head())\n",
    "\n",
    "best = cv_summary.iloc[0]\n",
    "print('\\nSelected hyperparameters:')\n",
    "print(best)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Final training on full training pool using best hyperparameters\n",
    "# ------------------------------------------------------------------\n",
    "bs = int(best['batch_size'])\n",
    "lr = float(best['lr'])\n",
    "epochs = int(best['epochs'])\n",
    "d_conv = float(best['dropout_conv'])\n",
    "d_head = float(best['dropout_head'])\n",
    "wd = float(best['weight_decay'])\n",
    "\n",
    "# create a new stratified split for validation during final training\n",
    "final_skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "train_idx, val_idx = next(final_skf.split(all_paths, all_labels))\n",
    "train_paths, val_paths = all_paths[train_idx], all_paths[val_idx]\n",
    "train_labels, val_labels = all_labels[train_idx], all_labels[val_idx]\n",
    "train_ds_full = make_ds_from_paths(train_paths, train_labels, IMG_SIZE, bs, augment=True, shuffle=True, seed=SEED)\n",
    "val_ds_full   = make_ds_from_paths(val_paths, val_labels, IMG_SIZE, bs, augment=False, shuffle=False)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "final_model = build_model_b_tunable(input_shape, num_classes, d_conv, d_head, wd)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "final_model.compile(optimizer=opt,\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "]\n",
    "history_final = final_model.fit(train_ds_full, validation_data=val_ds_full, epochs=epochs, verbose=2, callbacks=callbacks)\n",
    "\n",
    "# evaluate on test set\n",
    "# rebuild test_ds with best batch size to speed up evaluation\n",
    "from pathlib import Path as _Path\n",
    "_test_paths = sorted(str(p) for p in _Path('data/processed/test').glob('*/*.png'))\n",
    "_test_labels = []\n",
    "for idx, name in enumerate(class_names):\n",
    "    _test_labels += [idx] * len(list(_Path('data/processed/test').joinpath(name).glob('*.png')))\n",
    "_test_labels = np.array(_test_labels)\n",
    "test_ds = make_ds_from_paths(np.array(_test_paths), _test_labels, IMG_SIZE, bs, augment=False, shuffle=False)\n",
    "\n",
    "test_loss, test_acc = final_model.evaluate(test_ds, verbose=0)\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_ds.unbatch().batch(1024)], axis=0)\n",
    "y_prob = final_model.predict(test_ds, verbose=0)\n",
    "y_pred = y_prob.argmax(axis=1)\n",
    "print('Classification report (test set):')\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('Confusion matrix:\\n', cm)\n",
    "\n",
    "# save final model\n",
    "final_model.save('best_model_b_cv.keras')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Generate Markdown report\n",
    "# ------------------------------------------------------------------\n",
    "report_md = f\"\"\"\n",
    "### Model B cross-validated tuning\n",
    "- Selected hyperparameters: lr={lr}, epochs={epochs}, batch_size={bs}, dropout_conv={d_conv}, dropout_head={d_head}, weight_decay={wd}\n",
    "- CV val_acc mean ± std: {best.val_acc_mean:.4f} ± {best.val_acc_std:.4f}\n",
    "- CV val_loss mean ± std: {best.val_loss_mean:.4f} ± {best.val_loss_std:.4f}\n",
    "- Final test accuracy: {test_acc:.4f}\n",
    "Compliance: test set was held out and used only once for final evaluation.\n",
    "\"\"\"\n",
    "display(Markdown(report_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56eeb2",
   "metadata": {},
   "source": [
    "## Cross-validated hyperparameter tuning for Model B\n",
    "- **Procedure:** StratifiedKFold cross-validation (k=5, shuffling, seed=42) on the union of the training and validation splits. Augmentation is applied only on training folds, while validation and test data remain augmentation-free.\n",
    "- **Search space:** grid over learning_rate `[1e-4, 5e-4, 1e-3]`, epochs `[20, 30, 40]`, batch_size `[32, 64]`, dropout_conv `[0.10, 0.20]`, dropout_head `[0.25, 0.40]`, weight_decay `[1e-4, 5e-5]`. These ranges cover stable learning rates for Adam, reasonable training lengths with EarlyStopping, GPU-friendly batch sizes, and regularization knobs trading bias and variance.\n",
    "- **Selection:** highest mean validation accuracy (ties broken by lower mean validation loss, then preferring larger dropout or smaller weight decay).\n",
    "- **Results:** The code above prints a table of aggregated CV metrics, saves `cv_results_model_b.csv`, reports the best hyperparameters and their mean±std validation scores, retrains on the full training pool, and evaluates on the untouched test set with a classification report and confusion matrix. No information from the test set is used during tuning.\n"
